调用过程：(以 `basic_cnn` 为例)

```python
neunet.py ->
basic_cnn() ->

convnet = ConvNet(n_channel=3, n_classes=10, image_size=24)
```

参数：

```python
convnet = ConvNet(n_channel=3, n_classes=10, image_size=24)
n_channel: 通道数
n_classes: 暂时未知
image_size: 裁剪后的图片尺寸
```

生成一个卷积层

# 生成函数

## 卷积层生成函数:

应用了 weight decay (权值衰减)技术和 batch normalization (批正则化)技术

```python
class ConvLayer:
    ''' 卷积层生成函数

    '''

    def __init__(self, input_shape, n_size, n_filter, stride=1, activation='relu',
                 batch_normal=False, weight_decay=None, name='conv'):
        # params 初始化
        self.input_shape = input_shape
        self.n_filter = n_filter
        self.activation = activation
        self.stride = stride
        self.batch_normal = batch_normal
        self.weight_decay = weight_decay

        # 权重矩阵
        self.weight = tf.Variable(
            initial_value=tf.truncated_normal(
                shape=[  # 形状
                    n_size,
                    n_size,
                    self.input_shape[3],
                    self.n_filter],
                mean=0.0,
                stddev=numpy.sqrt(
                    2.0 / (
                        self.input_shape[1] *
                        self.input_shape[2] * self.input_shape[3]
                    )
                )  # 正态分布的标准差
            ),
            name='W_%s' % (name))

        # weight decay技术(权值衰减,防止过拟合)
        if self.weight_decay:
            weight_decay = tf.multiply(
                tf.nn.l2_loss(self.weight),
                self.weight_decay
            )
            tf.add_to_collection('losses', weight_decay)

        # 偏置向量
        self.bias = tf.Variable(
            initial_value=tf.constant(
                0.0, shape=[self.n_filter]),
            name='b_%s' % (name))

        # batch normalization 技术的参数
        if self.batch_normal:
            self.epsilon = 1e-5
            self.gamma = tf.Variable(
                initial_value=tf.constant(
                    1.0, shape=[self.n_filter]),
                name='gamma_%s' % (name))

          
    def get_output(self, input):
        # calculate input_shape and output_shape
        self.output_shape = [
            self.input_shape[0],
            int(self.input_shape[1]/self.stride),
            int(self.input_shape[2]/self.stride),
            self.n_filter]

        # hidden states
        self.conv = tf.nn.conv2d(
            input=input, filter=self.weight,
            strides=[1, self.stride, self.stride, 1], padding='SAME')

        # batch normalization 技术
        if self.batch_normal:
            mean, variance = tf.nn.moments(
                self.conv, axes=[0, 1, 2], keep_dims=False)
            self.hidden = tf.nn.batch_normalization(
                self.conv, mean, variance, self.bias, self.gamma, self.epsilon)
        else:
            self.hidden = self.conv + self.bias

        # activation
        if self.activation == 'relu':
            self.output = tf.nn.relu(self.hidden)
        elif self.activation == 'tanh':
            self.output = tf.nn.tanh(self.hidden)
        elif self.activation == 'none':
            self.output = self.hidden

        return self.output
```

## 池化层生成函数:

```python
class PoolLayer:
    '''池化层生成函数
    '''
    def __init__(self, n_size=2, stride=2, mode='max', padding='SAME',
                 resp_normal=False, name='pool'):
        # params
        self.n_size = n_size
        self.stride = stride
        self.mode = mode
        self.padding = padding
        self.resp_normal = resp_normal
        
    def get_output(self, input):
        if self.mode == 'max':
            self.pool = tf.nn.max_pool(
                value=input, ksize=[1, self.n_size, self.n_size, 1],
                strides=[1, self.stride, self.stride, 1], padding=self.padding)
        elif self.mode == 'avg':
            self.pool = tf.nn.avg_pool(
                value=input, ksize=[1, self.n_size, self.n_size, 1],
                strides=[1, self.stride, self.stride, 1], padding=self.padding)
        
        # response normal 技术
        if self.resp_normal:
            self.hidden = tf.nn.local_response_normalization(
                self.pool, depth_radius=7, alpha=0.001, beta=0.75)
        else:
            self.hidden = self.pool
            
        self.output = self.hidden
        
        return self.output
```

## `DenseLayer` (全连接层)生成函数

```python
class DenseLayer:
    
    def __init__(self, input_shape, hidden_dim, activation='relu', dropout=False, 
                 keep_prob=None, batch_normal=False, weight_decay=None, name='dense'):
        # params
        self.input_shape = input_shape
        self.hidden_dim = hidden_dim
        self.activation = activation
        self.dropout = dropout
        self.batch_normal = batch_normal
        self.weight_decay = weight_decay
        
        # 权重矩阵
        self.weight = tf.Variable(
            initial_value=tf.random_normal(
                shape=[self.input_shape[1], self.hidden_dim],
                mean=0.0, stddev=numpy.sqrt(2.0 / self.input_shape[1])),
            name='W_%s' % (name))
        
        # weight decay技术
        if weight_decay:
            weight_decay = tf.multiply(tf.nn.l2_loss(self.weight), self.weight_decay)
            tf.add_to_collection('losses', weight_decay)
            
        # 偏置向量
        self.bias = tf.Variable(
            initial_value=tf.constant(
                0.0, shape=[self.hidden_dim]),
            name='b_%s' % (name))
        
        # batch normalization 技术的参数
        if self.batch_normal:
            self.epsilon = 1e-5
            self.gamma = tf.Variable(
                initial_value=tf.constant(
                    1.0, shape=[self.hidden_dim]),
            name='gamma_%s' % (name))
        # dropout 技术
        if self.dropout:
            self.keep_prob = keep_prob
        
    def get_output(self, input):
        # calculate input_shape and output_shape
        self.output_shape = [self.input_shape[0], self.hidden_dim]
        # hidden states
        intermediate = tf.matmul(input, self.weight)
        
        # batch normalization 技术
        if self.batch_normal:
            mean, variance = tf.nn.moments(intermediate, axes=[0])
            self.hidden = tf.nn.batch_normalization(
                intermediate, mean, variance, self.bias, self.gamma, self.epsilon)
        else:
            self.hidden = intermediate + self.bias
            
        # dropout 技术
        if self.dropout:
            self.hidden = tf.nn.dropout(self.hidden, keep_prob=self.keep_prob)
            
        # activation
        if self.activation == 'relu':
            self.output = tf.nn.relu(self.hidden)
        elif self.activation == 'tanh':
            self.output = tf.nn.tanh(self.hidden)
        elif self.activation == 'softmax':
            self.output = tf.nn.softmax(self.hidden)
        elif self.activation == 'none':
            self.output = self.hidden
        
        return self.output
```

# 网络结构:

## 网络结构分析

所有卷积层和池化层均采用全零填充。

卷积核的步长均为1；核大小均为 `3x3` ；非线性修正函数均为 `relu` ；均采用权值衰减，权值衰减的参数为 `1e-4` ；均采用批正则化。

池化层的步长均为2；核大小均为 `2x2` ；均采用最大池化；均采用局部归一化(local response normalization)。 

卷积层1:

第一个卷积层将图片输入，创建 64 个卷积核。

```python
conv_layer1 = ConvLayer(
    input_shape=(None, image_size, image_size, n_channel),
    n_size=3, n_filter=64, stride=1,
    activation='relu', batch_normal=True,
    weight_decay=1e-4, name='conv1')
```

池化层1:

```python
pool_layer1 = PoolLayer(
    n_size=2, stride=2, mode='max', 
    resp_normal=True, name='pool1')
```

卷积层2:

128个卷积核。

```python
conv_layer2 = ConvLayer(
    input_shape=(None, int(image_size/2), 
    int(image_size/2), 64), n_size=3, n_filter=128,
    stride=1, activation='relu', batch_normal=True, 
    weight_decay=1e-4, name='conv2')
```

池化层2:

```python
pool_layer2 = PoolLayer(
    n_size=2, stride=2, mode='max', 
    resp_normal=True, name='pool2')
```

卷积层3:

256个卷积核

```python
conv_layer3 = ConvLayer(
    input_shape=(None, int(image_size/4), 
    int(image_size/4), 128), n_size=3, n_filter=256,
    stride=1, activation='relu', batch_normal=True, 
    weight_decay=1e-4, name='conv3')
```

池化层3:

```python
pool_layer3 = PoolLayer(
    n_size=2, stride=2, mode='max', 
    resp_normal=True, name='pool3')
```



`denselayer1`:

```python
dense_layer1 = DenseLayer(
    input_shape=(None, int(image_size/8) * int(image_size/8) * 256), 
    hidden_dim=1024, activation='relu', dropout=True, keep_prob=self.keep_prob,
    batch_normal=True, weight_decay=1e-4, name='dense1')
```

`denselayer2`:

```python
dense_layer2 = DenseLayer(
    input_shape=(None, 1024), hidden_dim=n_classes,
    activation='none', dropout=False, keep_prob=None,
    batch_normal=False, weight_decay=1e-4, name='dense2')
```

## 数据流

输入图片尺寸:

```bash
Tensor("images:0", shape=(?, 24, 24, 3), dtype=float32)
```

```python
hidden_conv1 = conv_layer1.get_output(input=self.images)
# Tensor("Relu:0", shape=(?, 24, 24, 64), dtype=float32)

hidden_pool1 = pool_layer1.get_output(input=hidden_conv1)
# Tensor("LRN:0", shape=(?, 12, 12, 64), dtype=float32)

hidden_conv2 = conv_layer2.get_output(input=hidden_pool1)
# Tensor("Relu_1:0", shape=(?, 12, 12, 128), dtype=float32)

hidden_pool2 = pool_layer2.get_output(input=hidden_conv2)
# Tensor("LRN_1:0", shape=(?, 6, 6, 128), dtype=float32)

hidden_conv3 = conv_layer3.get_output(input=hidden_pool2)
# Tensor("Relu_2:0", shape=(?, 6, 6, 256), dtype=float32)

hidden_pool3 = pool_layer3.get_output(input=hidden_conv3)
# Tensor("LRN_2:0", shape=(?, 3, 3, 256), dtype=float32)

input_dense1 = tf.reshape(
    hidden_pool3, 
    [-1, int(image_size/8) * int(image_size/8) * 256]
)
output_dense1 = dense_layer1.get_output(input=input_dense1)
logits = dense_layer2.get_output(input=output_dense1)
```

